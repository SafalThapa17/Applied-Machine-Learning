{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1jLUbDgwKw/QE5lh/2oQr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SafalThapa17/Applied-Machine-Learning/blob/main/HW2_Problem_6_Safal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Safal Thapa\n",
        "\n",
        "Empid (last 5 digit): 30789\n",
        "\n",
        "HW2-Problem 6: Backpropagation Proofs and Implementation (Nielsen)"
      ],
      "metadata": {
        "id": "s9l2FWniVG25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part a: Proof of Equation BP3\n",
        "\n",
        "Write out a complete proof of equation BP3 from Nielsen's chapter (the backpropagation equation for the output layer).\n",
        "\n"
      ],
      "metadata": {
        "id": "4vZaJ2weU4BM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<center> Answer <center/>"
      ],
      "metadata": {
        "id": "Dm6BnvEgWlhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An equation for the rate of change of the cost with respect to any bias in the network:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial C}{\\partial b^l_{j}} = \\delta^l_{j}\n",
        "$$\n",
        "\n",
        "For Proof:\n",
        "We consider a neuron j in layer l.\n",
        "We have Weighted input:\n",
        "$$\n",
        "z_j^{\\,l} = \\sum_k w_{jk}^{\\,l}\\, a_k^{\\,l-1} + b_j^{\\,l}\n",
        "$$\n",
        "\n",
        "Activation: $\n",
        "a_j^{\\,l} = \\sigma\\!\\left(z_j^{\\,l}\\right)\n",
        "$\n",
        "\n",
        "Cost function: C\n",
        "\n",
        "The bias $b^l_{j}$ affects the cost only through $z^l_{j}$  so we will apply the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial C}{\\partial b_j^{\\,l}}\n",
        "=\n",
        "\\frac{\\partial C}{\\partial z_j^{\\,l}}\n",
        "\\frac{\\partial z_j^{\\,l}}{\\partial b_j^{\\,l}}\n",
        "\\tag{a}\n",
        "$$    (a)\n",
        "\n",
        "\n",
        "From the definition, we have\n",
        "$\n",
        "z_j^{\\,l} = \\sum_k w_{jk}^{\\,l}\\, a_k^{\\,l-1} + b_j^{\\,l}\n",
        "$.\n",
        "\n",
        "Taking the derivative with respect to $b_j^{\\,l}$. The sum $b_j^{\\,l} = \\sum_k w_{jk}^{\\,l}\\, a_k^{\\,l-1}$ does not involve the bias $b_j^{\\,l}$ so the derivative is 0. The bias term $b_j^{\\,l}$ is added directly and its derivative is 1. Hence,\n",
        "$$\n",
        "\\frac{\\partial z_j^{\\,l}}{\\partial b_j^{\\,l}} = 1\n",
        "\\tag{b}$$     \n",
        "\n",
        "Recalling the definition, the error for neuron j in layer l is\n",
        "\n",
        "$$\n",
        "\\delta_j^{\\,l} = \\frac{\\partial C}{\\partial z_j^{\\,l}}\n",
        "\\tag{c}$$\n",
        "\n",
        "Substituting (b) and (c) into equation (a), we get\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial C}{\\partial b_j^{\\,l}}\n",
        "&= \\frac{\\partial C}{\\partial z_j^{\\,l}} \\frac{\\partial z_j^{\\,l}}{\\partial b_j^{\\,l}} \\\\\n",
        "&= \\frac{\\partial C}{\\partial z_j^{\\,l}} \\cdot 1 \\\\\n",
        "&= \\delta_j^{\\,l}\n",
        "\\end{aligned}\n",
        "$$\n",
        "which is BP3.\n",
        "\n",
        "Hence, proved."
      ],
      "metadata": {
        "id": "P_AUsDNSWppt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part b: Proof of Equation BP4**\n",
        "\n",
        "Write out a complete proof of equation BP4 from Nielsen's chapter (the backpropagation equation for hidden layers)."
      ],
      "metadata": {
        "id": "_3agwlSJvcTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<center> Answer <center/>"
      ],
      "metadata": {
        "id": "buUs9LBPvfPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An equation for the rate of change of the cost with respect to any weight in the network:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial C}{\\partial w_{jk}^{\\,l}} = a_k^{\\,l-1} \\, \\delta_j^{\\,l}\n",
        "$$\n",
        "\n",
        "Proof:\n",
        "The bias $w^l_{jk}$ affects the cost only through $z^l_{j}$. We apply the chain rule\n",
        "\n",
        "$$\n",
        "\\frac{\\partial C}{\\partial w_{jk}^{\\,l}}\n",
        "=\n",
        "\\frac{\\partial C}{\\partial z_j^{\\,l}}\n",
        "\\cdot\n",
        "\\frac{\\partial z_j^{\\,l}}{\\partial w_{jk}^{\\,l}}\n",
        "\\tag{d}\n",
        "$$\n",
        "\n",
        "Taking partial derivative with respect to $w^l_{jk}$. We get\n",
        "$$\n",
        "\\frac{\\partial z_{j}^{\\,l}}{\\partial w_{jk}^{\\,l}} = a_k^{\\,l-1}\n",
        "\\tag{e}\n",
        "$$\n",
        "\n",
        "By back propagation,\n",
        "$$\n",
        "\\delta_j^{\\,l} = \\frac{\\partial C}{\\partial z_j^{\\,l}}\n",
        "\\tag{f}\n",
        "$$\n",
        "\n",
        "Substituting (e) and (f) into (d), we get\n",
        "$$\n",
        "\\frac{\\partial C}{\\partial w_{jk}^{\\,l}}\n",
        "=\n",
        "a_k^{\\,l-1}\\cdot\n",
        "\\delta_j^{\\,l}\n",
        "$$\n",
        "\n",
        "Which is BP4. Hence, proved.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HzCRF4qvv4EV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part c: Matrix-Based Backpropagation Implementation**\n",
        "\n",
        "Implement a fully matrix-based backpropagation algorithm over a mini-batch: - Augment input variables with a \"column\" of 1s (instead of a separate bias term) - Treat bias as weight w_0 - Use Nielsen's code as a starting point, but rewrite it to use matrix notation (no separate bias) - Test on the Iris dataset with: - 4 input features (5 with the constant column) - 3 hidden layer nodes - 3 output nodes (one per class)"
      ],
      "metadata": {
        "id": "CgzcyigmxF1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        # add a column to each weight matrix\n",
        "        self.weights = [np.random.randn(y, x + 1)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for w in self.weights:\n",
        "            a_aug = np.vstack([np.ones((1, a.shape[1])), a])\n",
        "            a = sigmoid(np.dot(w, a_aug))\n",
        "        return a\n",
        "\n",
        "    def update_mini_batch(self, mini_batch_x, mini_batch_y, eta):\n",
        "        m = mini_batch_x.shape[1]\n",
        "\n",
        "        # Call backprop once for the entire matrix\n",
        "        nabla_w = self.backprop(mini_batch_x, mini_batch_y)\n",
        "\n",
        "        # Update weights using the gradients\n",
        "        self.weights = [w - (eta / m) * nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Matrix-based version of backpropagation.\"\"\"\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # --- Feedforward ---\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "\n",
        "        for w in self.weights:\n",
        "            # Augment input with a row of 1s\n",
        "            a_aug = np.vstack([np.ones((1, activation.shape[1])), activation])\n",
        "            z = np.dot(w, a_aug)\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "\n",
        "        # Gradient for the last layer (using augmented activations from layer L-1)\n",
        "        a_prev_aug = np.vstack([np.ones((1, activations[-2].shape[1])), activations[-2]])\n",
        "        nabla_w[-1] = np.dot(delta, a_prev_aug.transpose())\n",
        "\n",
        "        # Backprop through hidden layers\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "\n",
        "            # slice out the w0 (bias) weights when\n",
        "            # backpropagating error to previous layers\n",
        "            delta = np.dot(self.weights[-l+1][:, 1:].transpose(), delta) * sp\n",
        "\n",
        "            # Gradient for layer -l\n",
        "            a_prev_aug = np.vstack([np.ones((1, activations[-l-1].shape[1])), activations[-l-1]])\n",
        "            nabla_w[-l] = np.dot(delta, a_prev_aug.transpose())\n",
        "\n",
        "        return nabla_w\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations - y)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "\n",
        "# Load and prepare data\n",
        "iris = load_iris()\n",
        "X = iris.data.T\n",
        "y_labels = iris.target.reshape(-1, 1)\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "Y = encoder.fit_transform(y_labels).T\n",
        "\n",
        "# 4 inputs, 3 hidden, 3 outputs\n",
        "net = Network([4, 3, 3])\n",
        "\n",
        "# Training\n",
        "print(\"Training Model...\")\n",
        "for epoch in range(1001):\n",
        "    net.update_mini_batch(X, Y, eta=0.3)\n",
        "    if epoch % 200 == 0:\n",
        "        outputs = net.feedforward(X)\n",
        "        acc = np.mean(np.argmax(outputs, axis=0) == np.argmax(Y, axis=0))\n",
        "        print(f\"Epoch {epoch}: Accuracy {acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk0Y1Y-pM4TR",
        "outputId": "1e89779b-91c1-4d02-9879-f7e285472140"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model...\n",
            "Epoch 0: Accuracy 33.33%\n",
            "Epoch 200: Accuracy 74.67%\n",
            "Epoch 400: Accuracy 96.00%\n",
            "Epoch 600: Accuracy 98.00%\n",
            "Epoch 800: Accuracy 98.00%\n",
            "Epoch 1000: Accuracy 97.33%\n"
          ]
        }
      ]
    }
  ]
}